**ğŸ™ï¸ Local Voice Assistant (STT â†’ LLM â†’ TTS)**

A fully local, real-time voice assistant pipeline that converts speech â†’ text â†’ response â†’ speech, designed to run on affordable hardware (low RAM, optional GPU) with no API keys, no token limits, and full privacy.

**âœ¨ Features**

ğŸ”Š Real-time Speech-to-Text (STT) using faster-whisper

ğŸ§  Local LLM inference (1â€“2B or smaller models)

ğŸ—£ï¸ Local Text-to-Speech (TTS) (interruptible)

ğŸ§ Microphone streaming with VAD (Voice Activity Detection)

ğŸ” Turn-taking & barge-in support

ğŸ§© Modular, extensible architecture

ğŸ”’ Fully offline â€“ no cloud, no billing, no telemetry

**ğŸ§  System Overview**

Microphone
   â†“
Audio Capture & Preprocessing
   â†“
VAD + Streaming Buffer
   â†“
STT (faster-whisper, int8)
   â†“
Dialogue Manager
   â†“
LLM (local small model)
   â†“
TTS (local)
   â†“
Speaker


*The system supports streaming partial transcripts, final utterance detection, and interrupting TTS when the user speaks.*

**ğŸ—‚ï¸ Project Structure**
```
voice-assistant/
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”œâ”€â”€ mic_input.py
â”‚   â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”‚   â”œâ”€â”€ ring_buffer.py
â”‚   â”‚   â””â”€â”€ audio_output.py
â”‚   â”œâ”€â”€ stt/
â”‚   â”‚   â”œâ”€â”€ vad.py
â”‚   â”‚   â”œâ”€â”€ stt_engine.py
â”‚   â”‚   â”œâ”€â”€ streaming_stt.py
â”‚   â”‚   â””â”€â”€ text_stabilizer.py
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ llm_engine.py
â”‚   â”‚   â”œâ”€â”€ prompt_manager.py
â”‚   â”‚   â””â”€â”€ memory.py
â”‚   â”œâ”€â”€ tts/
â”‚   â”‚   â”œâ”€â”€ tts_engine.py
â”‚   â”‚   â””â”€â”€ tts_queue.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ timing.py
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ run_local.py
    â”œâ”€â”€ test_mic.py
    â”œâ”€â”€ test_stt.py
    â”œâ”€â”€ test_llm.py
    â””â”€â”€ test_tts.py
```
**ğŸ“¦ Dependencies**
Core libraries
```
pip install \
  faster-whisper \
  sounddevice \
  pyaudio \
  numpy \
  soxr \
  webrtcvad-wheels
```

**Optional (recommended)**

torch â€“ if using Silero VAD or certain TTS engines

llama-cpp-python â€“ for local LLM inference

fastapi / websockets â€“ if exposing a service

***Audio Requirements***

All audio is normalized to:
* Sample rate: 16,000 Hz
* Channels: Mono
* Frame size: 20â€“30 ms
* Format: PCM int16 or float32
* Resampling is handled automatically using soxr.

**STT Pipeline (Streaming Mode)**
* Microphone frames captured continuously
* VAD detects speech activity
* Rolling buffer (5â€“10 seconds)
* STT runs every 200â€“500 ms during speech

*Emits:*

Partial transcripts (live)
Final transcript after silence timeout
Recommended STT settings (low hardware)
Model: tiny or base
compute_type="int8"

beam_size=1

VAD aggressiveness: 2â€“3

**LLM Pipeline**
* Small local model (â‰¤2B parameters)
* Short conversational memory (last N turns)
* Optimized for spoken responses
* No fine-tuning required for basic conversation
* Typical usage
* Triggered only after final STT text
* Optional token streaming
* Short, natural replies (voice-friendly)

**TTS Pipeline**
* Local TTS engine (e.g. Piper or equivalent)
* Sentence-level chunking
* Playback queue
* Immediate stop on barge-in
* Required TTS features
* speak(text)
* stop()
* is_speaking()

**ğŸ’» Hardware Targets**

* Designed to run on:

* 4â€“8 GB RAM

* CPU-only or low-end GPU

* Laptop / mini-PC / edge device

*Performance tips*
* Keep LLM context short
* Use quantized STT models
* Stream audio & text
* Avoid running all heavy tasks simultaneously

Author: Nam Nhat