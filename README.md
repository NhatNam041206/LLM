**ğŸ™ï¸ Local Voice Assistant (STT â†’ LLM â†’ TTS)**

A fully local, real-time voice assistant pipeline that converts speech â†’ text â†’ response â†’ speech. It is designed to run on modest hardware (4â€“8 GB RAM) and keeps all processing offline.

âœ¨ Features
- Streaming STT with VAD and rolling buffer (`src/stt/streaming_stt.py`)
- Local LLM inference via `llama-cpp-python` (`src/llm/llm_engine.py`)
- Local TTS via Piper (`src/tts/tts_engine.py`)
- Modular components: `audio/`, `stt/`, `llm/`, `tts/`

Project layout (key files)
```
./
â”œâ”€ README.md
â”œâ”€ src/
â”‚  â”œâ”€ audio/
â”‚  â”‚  â”œâ”€ mic_input.py
â”‚  â”‚  â”œâ”€ frame_aligner.py
â”‚  â”‚  â””â”€ ring_buffer.py
â”‚  â”œâ”€ stt/
â”‚  â”‚  â”œâ”€ stt_engine.py
â”‚  â”‚  â”œâ”€ streaming_stt.py
â”‚  â”‚  â””â”€ vad.py
â”‚  â”œâ”€ llm/
â”‚  â”‚  â”œâ”€ llm_engine.py
â”‚  â”‚  â””â”€ prompt_manager.py
â”‚  â””â”€ tts/
â”‚     â”œâ”€ tts_engine.py
â”‚     â””â”€ audio_player.py
â””â”€ scripts/
   â”œâ”€ test_tts_once.py
   â”œâ”€ test_tts_chatloop.py
   â”œâ”€ test_stt_streaming.py
   â””â”€ test_llm.py
```

Dependencies
------------
Install required packages (core):
```bash
pip install -r requirements.txt
```
If installing manually, the main packages are:
```bash
pip install faster-whisper sounddevice numpy soxr webrtcvad-wheels
pip install llama-cpp-python   # for LLM via llama.cpp bindings
pip install piper             # if using Piper TTS package
```

Notes on optional/OS-specific deps
- `pyaudio` is optional; `sounddevice` is the primary playback/capture library used here.
- `torch` is only required for optional components (not used in main flow).

Audio requirements
------------------
- Internal standard: 16 kHz, mono, 20 ms frames, float32.
- STT components expect 16 kHz mono windows; resampling is handled automatically.

Model setup (Important!)
------------------------
**Note:** The `models/` folder is **not** published in this repository. You must download the required models manually.

**TTS Model (Piper)**
- Download from: https://huggingface.co/rhasspy/piper-voices/tree/main/en/en_US/amy/medium
- Files needed: `en_US-amy-medium.onnx` and `en_US-amy-medium.onnx.json`
- Place in: `models/tts/`
- Usage: Update `model_path` in `scripts/test_tts_once.py` to point to the `.onnx` file

**LLM Model (Llama via llama.cpp)**
- Download from: https://huggingface.co/hugging-quants/Llama-3.2-1B-Instruct-Q4_K_M-GGUF/tree/main
- File needed: `Llama-3.2-1B-Instruct-Q4_K_M.gguf` (or similar GGUF quantization)
- Place in: `models/llm/` (or any accessible path)
- Usage: Update `model_path` in LLM config to point to the `.gguf` file

After downloading, your directory should look like:
```
./models/
â”œâ”€ tts/
â”‚  â”œâ”€ en_US-amy-medium.onnx
â”‚  â””â”€ en_US-amy-medium.onnx.json
â””â”€ llm/
   â””â”€ Llama-3.2-1B-Instruct-Q4_K_M.gguf
```

Quick start
-----------
1. Ensure Python v3.9+ and required packages installed.
2. Point model paths in `scripts/test_tts_once.py` or other scripts.
3. Run example scripts:
```bash
python scripts/test_tts_once.py      # synth + playback example
python scripts/test_tts_chatloop.py # interactive TTS loop
python scripts/test_stt_streaming.py# streaming STT demo
```

Where to look next
------------------
- `src/stt/` â€” streaming coordinator, VAD, ring buffer
- `src/llm/` â€” prompt manager and simple LLM engine (llama.cpp backend)
- `src/tts/` â€” Piper TTS wrapper and player
- `src/tts/instruction_tts.md`, `src/stt/instruction_stt.md`, `src/llm/instruction_llm.md` â€” human-facing instructions for each subsystem

Troubleshooting
---------------
- If audio is silent for TTS: check `model_path` and presence of `.json` config file next to the ONNX model.
- If STT returns empty transcripts: confirm microphone device, sample rate, and VAD aggressiveness.

Author: Nam Nhat